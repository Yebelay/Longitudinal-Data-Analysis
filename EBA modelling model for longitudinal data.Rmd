---
title: |
     <span style="color:white"> Longitudinal Analysis Workshop </span>

author: <span style="color:#38BDDE"> Anteneh Tesema and Yebelay Berehan </span>
  
institute: |
           <span style = 'font-size: 80%;'>  Ethiopian Public Health Institute (EPHI) 
           National Data Management Center for Health (NDMC)</span>
           
date: |
      <span style = 'font-size: 50%;'> `r Sys.Date()`
      
output:
  xaringan::moon_reader:
    css: [default,  xaringan-themer.css, rladies-fonts]
    nature:
     # highlightStyle: googlecode
      highlightLines: true
      #highlightLanguage: ["r"]
      ratio: "14:9"
      highlightSpans: true
      highlightStyle: github
      countIncrementalSlides: false
      titleSlideClass: ["center","middle"]
  includes:
    in_header: columns.tex
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(here)
library(snakecase)
library(DT)
library(naniar)
#library(kableExtra)
```

```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_logo(image_url = "Slides/Images/tidyverse.png", width = "95px",height = "95px")
```

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
mono_light(
  base_color =  "#151B54", ## OHSU Marquam
  code_highlight_color = "#c0e8f5",
  link_color = "#38BDDE",
  background_color = "#FFFFFF",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i","400i","700"),
  code_font_google   = NULL,
  text_slide_number_color = "#FF0266",
  code_inline_color = "#0000FF",
  #inverse_background_color = "#272822",
  #title_slide_background_color = inverse_background_color,
 # code_font_size = "0.6em", 
  footnote_color = "blue",
  #footnote_position_bottom = "0.1em"
  )
```


##	Topics covered 

 1. ** <span style="color:purple">Day 1: Introduction to Longitudinal Analysis</span>**
  * Understanding <span style="color:red">Non-Independence</span> in Clustered Data
  * <span style="color:red">Limitations of Classical Analysis Methods</span>
  * Exploratory Analysis of Marginal Distribution (Average Evolution, Variance Structure, and Correlation Structure)
 2. ** <span style="color:purple">Day 2: Methods for Continuous Outcomes</span>**
  *	Linear Mixed Effects Models
  * Marginal Models
  * Hierarchical Models
  * Multilevel Models
 3. ** <span style="color:purple">Day 3: Methods for Discrete Data</span> **
  * Generalized Estimating Equations (GEE)
  * Generalized Linear Mixed Models (GLMM)
 4. ** <span style="color:purple">Day 4: Addressing Missing Data in Longitudinal Studies</span>**
  * Types of Missing Data Mechanisms (MCR, MR, NMR)
  * Handling Missing Data: Multiple Imputation
  * Handling Missing Data: Weighted GEE

---

## Goal of the Workshop

The goal of this course is to:

- Provide an overview of fundamental statistical models and methods for the analysis of **longitudinal data**, including key theoretical results presented.

- Focus on the practical implementation of these methods in **R **.

- Help **trainees** gain a comprehensive understanding of the properties and use of modern methods for **longitudinal data analysis**.

- Enable trainees to pose scientific questions within the context of appropriate statistical models and **carry out and interpret analyses effectively**.

---

## Primary Objectives of the Workshop

- Understand the effect of non-independence in longitudinal data.

- Recognize limitations of classical analysis methods in longitudinal studies.

- Explore and analyze the marginal distribution of longitudinal data.

- Learn and apply methods for analyzing continuous outcomes using linear mixed effects models.

- Utilize methods for analyzing discrete data in longitudinal studies using GEE and GGLMM.

- Gain knowledge about missing data mechanisms and techniques for handling them.


---
---

class: inverse, middle

# Day 1: Introduction to Longitudinal Analysis
  
* Understanding <span style="color:red">Non-Independence</span> in Clustered Data
* <span style="color:red">Limitations of Classical Analysis Methods</span>
* Exploratory Analysis of Marginal Distribution (Average Evolution, Variance Structure, and Correlation Structure)

---

## Introduction to Longitudinal Analysis

**<span style="color:purple">Longitudinal data:</span>** Data in the form of repeated measurements over time or other factors on each individual or unit in a sample from a population of interest.

Examples:
- Weekly measurements of growth on experimental plots with different fertilizers.
- Monthly measurements of viral load on HIV-infected patients with different treatment regimens.

**Defining Characteristic**

The same response or outcome is measured repeatedly on each unit.

**Scientific Questions**

- How mean response differs across treatments or other factors.
- How the change in mean response over time differs.
- Other features of the relationship between response/outcome and time.

---

## Required Statistical Model

A statistical model that acknowledges this data structure in which the questions can be formalized and associated specialized methods of analysis based on the model.

- Longitudinal data studies have become increasingly common and widespread across various scientific disciplines.
- We will study both classical and more modern approaches to representing and interpreting these data.

**Terminology**

- Longitudinal data refers to data in the form of repeated measurements that might be over time but could also be over some other set of conditions.
- Time is most often the condition of measurement.
- "Response" and "outcome" are used interchangeably to denote the repeated measurement or outcome of interest.
- "Unit," "individual," and "subject" are used interchangeably to refer to the entity being measured.

**Applications**

Next, we consider several applications that exemplify longitudinal data situations and the range of ways data are collected and types of responses and questions of interest.


---
class: inverse, middle

# Day 2: Methods for Continuous Outcomes

*	Linear Mixed Effects Models
* Marginal Models
* Hierarchical Models
* Multilevel Models


---
class: inverse, middle


# Day 3: Methods for Discrete Data

* Generalized Estimating Equations (GEE)
* Generalized Linear Mixed Models (GLMM)
  
---
class: inverse,  middle

# Day 4: Missing Data Management

* What is missing data â€“ definition, patterns, mechanisms (MCR, MR, NMR)

* Simple methods for handling missing data

* Multiple Imputation (MI) based procedures

* Weighted GEE


---

## Introduction

- Missing data is very common in statistical analysis. 
  - is a problem that commonly encounter in research.

- In longitudinal studies, not all individuals are observed at all time points or visits.

- Some individuals may be observed only at one time point, or at first two or three, and so on.

- Having too much missing data can invalidate a study because

 1. It reduces statistical power, 
 
 2. causes bias in estimation of parameters,
 
 3. reduces the representatives of the samples, and 
 
 4. it complicates analyses of the studies. 

---

## What to consider

- Dealing with missing data requires considering the missing data **patterns**, **mechanisms**, **proportion** and the chosen analytic approach.

- **Missing Data Pattern**: It is essential to examine the missing data pattern, such as whether it follows a monotone, intermittent, or arbitrary structure. Understanding the pattern can guide the selection of suitable methods.

- **Proportion of Missing Data**: The proportion of missing data in the dataset is crucial. High levels of missingness may impact the validity of analyses and may require more sophisticated handling techniques.

- **Reasons for Missing Data**: Understanding the reasons why data are missing is critical. Missingness can occur due to various factors like participant dropouts, measurement errors, or system malfunctions. Identifying the reasons can help mitigate potential biases.

- ensure robust analyses in longitudinal studies when there is missing data.
---

## Missing Data in Longitudinal Studies


**Monotone Missing Data Pattern**

- In longitudinal studies, missing data patterns can have various structures. 

- One common pattern is the monotone missing data pattern, where not all individuals are observed at all time points, leading to dropouts in the data. 

The table representing the monotone missing data pattern for a longitudinal studies:

| Study | Time1 | Time2 | Time3 |
|-------|-------|-------|-------|
| 1     | X     | X     | X     |
| 2     | X     | X     | .     |
| 3     | X     | .     | .     |

---

**Intermittent/Arbitrary Missing Data Pattern**

- In longitudinal studies, another common missing data pattern is the intermittent or arbitrary pattern, where data is missing at different time points for various individuals. 


The table representing the arbitrary missing data pattern for longitudinal studies:

| Study | Time1 | Time2 | Time3 |
|-------|-------|-------|-------|
| 1     | X     | X     | X     |
| 2     | X     | .     | X     |
| 3     | .     | X     | X     |
| 4     | .     | .     | X     |
| 5     | .     | X     | .     |


- Proper data handling, including data imputation, is crucial to ensure reliable subsequent data analysis.

---

## Types of Missing Data

- Looking carefully the causes of missingness enable us to employee the **appropriate missing data management system**.
  - Estimation of the parameter with missing data depends on the missing data mechanism. 

- The missing data mechanism is a probability model for missingness. 
- Data is often described in accordance to **the reasons for the missing data**. 

- According to the mechanisms of missingness, we assume **three types** of missing data.
  - Missing Completely at Random (MCAR)
  - Missing at Random (MAR)
  - Not Missing at Random (NMAR)

---

### 1) Missing Completely at Random (MCAR)

- In MCAR missingness is assumed to be independent on both observed and unobserved data. 

- The notation of MCAR mechanism is expressed as: $$p(R|Y)=p(R|Y^{obs},Y^{mis})=p(R)$$ 
  - Y is a vector of partially observed data, that is, $Y=(Y^{obs},Y^{mis})$, 
  - R is a set of missing indicators (i.e., R= 1 if the $j^{th}$ element of Y is observed, and R= 0 if the $j^{th}$ element of Y is missing) Rubin (1976). 

- Called ignorable missing  in statistical inference. 

- Examples of such MCAR data include data that are missing by design, due to equipment failure, samples lost in transit, and technically unsatisfactory.

- An advantage of MCAR is that its data does not affect any bias in statistical analyses. 

- Though, statistical power is decreased, any estimated parameters are not biased as a result of the missing data.

---

### 2) Missing at Random (MAR)

- Missingness depends only on observed components $(Y^{obs})$, not on missing components $Y^{mis}$,
    - i.e expressed through the formula: 
    $$p(R|Y^{obs},Y^{mis})=p(R|Y^{obs}).$$

- The missing data mechanism also can be ignored in likelihood inference. 

- MAR is not problematic because it does not produce statistical bias. 

- For example, if a child does not attend an educational assessment because the child is ill, this might be predictable from other data we have about the child's health, but it would not be related to what we would have measured had the child not been ill.

---

### 3) Not Missing at Random (NMAR)

- MNAR suggests that the probability of a value being missing fluctuates for reasons unknown to us. 

- When the characteristics of missing data do not meet those of MCAR and MAR, they are categorised into data that is MNAR. 

- The MNAR depends on both the observed value and the missing values, $$p(R|Y^{obs},Y^{mis}) =p(R|Y^{obs}, Y^{mis}).$$ 

- In this case the missing data mechanism is non-ignorable, so likelihood inference must incorporate the missing data model $p(R|Y^{mis})$.
- Example: a student skips a tutorial lesson because the student knows that the attendance would not be graded.
- **Often difficult to diagnose and handle**.

---

### Common Methods for dealing with missing data

**Ad-hoc Methods**

- A number of theories were introduced to account missing data.

1. **Listwise Deletion**

- The most commonly used approach that data scientists use to deal with missing data is to simply omit cases with missing data, only analysing the rest of the dataset. 
- This method is known as listwise deletion or complete-case analysis.

- The **<span style="color:magenta"> na.omit()</span>** function in R removes all cases with one or more missing data values in a dataset.

- Letâ€™s create a new small dataframe as an example to demonstrate Listwise Deletion

---

## Ad-hoc Methods

```{r, collapse = TRUE}
Name <- c("John", "Tim", NA)
Sex <- c("men", "men", "women")
Age <- c(45, 53, NA)
df <- data.frame(Name, Sex, Age)
```



---


**3. Mean Imputation**

- Some data scientists or statisticians may look for a quick fix by replacing missing data with the mean.
- Mode is often used to impute categorical data.
- We use `mice package` with argument **<span style="color:green">method = mean</span>**, & **<span style="color:red">m = 1</span>.** 

```{r, warning=F, message=FALSE}
# install.packages("mice")
library(mice)
imp <- complete(mice(airquality, method = "mean", 
                     m = 1, maxit = 10))
View(imp)
```

---

```{r, warning=F, message=FALSE, fig.width=6, fig.height=3}
#densityplot(imp)
```

- The density of the imputed data for both variables is shown in <span style="color:red">red</span>, while the density of the observed data is shown in <span style="color:red">blue</span>.

---

**4. Regression Imputation**

- This is done by first building a linear model of the observed data. The missing data are then predicted using the fitted model.
- The cases of MNAR data are problematic.
- Letâ€™s try the regression imputation method on the airquality dataset!

```{r}
library(mice)
# Impute using linear regression prediction
airqualityLm <- complete(mice(data = airquality, 
                              m = 5, method = 
                                c("norm.predict")))
# View(airqualityLm)
```

---

**5. Last Observation Carried Forward (LOCF)**

- LOCF is an imputation method for longitudinal data, which involves taking the previous observed value to replace missing values in the dataset.
- This method is especially useful for data to be plotted for time series analysis.
- The LOCF can be seen as a good method of choice because it produces a complete dataset and it can be used for cases where we know what the missing values should be.
- To perform the LOCF imputation method, we simply run the 
  - <span style="color:orange">fill()</span> function from the tidyr package on the dataset and the variable with the missing data.
  - <span style="color:cyan">lna.locf()</span> function in zoo

---

## Multiple Imputation

- Multiple imputation is a process that is done in 3 main steps: **<span style="color:blue"Imputation, analysis, and pooling.</span>**
- This gives the imputed data a valid statistical inference.

<span style="color:red">Steps for Multiple Imputation:</span>

- Firstly, generate m multiple imputed datasets.
- Secondly, analyze each imputed dataset, then there should be m analyses.
- Lastly, combine the results for the pooled dataset.
- Multiple imputation is robust to small sample sizes or lots of missing data.

---

## Multiple Imputation

- Steps in applying multiple imputation to missing data via the `mice` approach

```{r savedplot, out.width='60%', fig.align='center', echo=FALSE}
knitr::include_graphics("Image/figMI.png")
```

---

## Multiple Imputation

- The estimate of the parameter $\beta$ is simply the average of each parameter estimate $\beta^m$ obtained over the m imputed datasets (m = 1, ..., M):$$\hat{\beta}^* = \frac{1}{M}\sum_{m=1}^M \hat{\beta}^m$$
- The variance of the estimator is partitioned into within imputation variance (sampling variability), and the between imputation variance (estimation variability due to missing data).

---

## Multiple Imputation

- The within imputation variance, $W_{\beta}$, over the m imputed datasets is:
$$W_{\beta}= \frac{\sum_{m=1}^M SE_{\beta}^2}{M}$$
- The between imputation variance, $B_{\beta}$, over the m imputed datasets is: $$B_{\beta}= \frac{\left(\sum_{m=1}^M\left(\hat{\beta}^m-\hat{\beta}^*\right)^2\right)}{M-1}$$

- These two variances are combined to provide a single variance, given by
$$T_{\beta}=W_{\beta}+ \left[\frac{(M+1)}{M}\right] B_{\beta}$$

---

## con't

- Let's look at it using the mice() function 

```{r}
impData <- mice(airquality, m = 3, 
                maxit = 50, meth = 'pmm',
                seed = 100, print = FALSE)
#impData$m; impData$nmis; impData$iteration; impData$imp
# View(impData$imp$Ozone); summary(impData); plot(impData)
```

---

## con't 

Now we can extract the completed dataset using the complete() function. 

```{r, warning=FALSE, message=FALSE}
#library(tidyr)
impdata <- mice::complete(impData, action = "long", inc = F)
View(impdata)
#View(airquality)
```

The missing values have been replaced with the imputed values in the first of the five datasets. If you wish to use another one, just change the second parameter in the <span style="color:red">complete()</span> function.

---

## con't

```{r, warning=F, message=FALSE, fig.width=6, fig.height=3}
densityplot(impData)
```

- The density of the imputed data for each imputed dataset is shown in magenta while the density of the observed data is shown in blue.

---

## con't

**Pooling**

- Suppose that the next step in our analysis is to fit a linear model to the data.
- You may ask what imputed dataset to choose.
- The mice package makes it easy to fit a model to each of the imputed datasets and then pool the results together

```{r}
# pool(with(impData, lm(Temp~ Ozone+ Solar.R+Wind)))
#summary(fit)
# summary(pool(fit))
# pool.r.squared(fit, adjusted = TRUE)
```

---

## Multiple Imputation Software 

- **`Amelia`** in R (by Gary King and collaborators)
- **`mi`** in R (by Andrew Gelman and collaborators)
- **`mice`** in R (by Stef van Buuren and collaborators) 
- SPSS **`(Analyze > Multiple Imputation)`**
- STATA  **mi `estimate`**

---

## Expectation-Maximization (EM)

- Expectation-Maximization is a likelihood-based method with three steps: 
   1) estimate parameters using observed data; 
   2) use the estimates to create a regression equation that is used to impute the missing values, then 
    3) fill-in the missing data. 
- However, this sometimes takes time to converge especially when there is too much missing data.

---

## Expectation-Maximization (EM)

- It does this by iterating between the E and M steps. 
```{r, warning=FALSE, message=FALSE, fig.height=3, fig.align='center'}
library(mvdalab)
result <- imputeEM(airquality)
```

---

# Conclusion

- In conclusion, researchers should always understand the nature missingness of the data (MCAR, MAR or MNAR), then deal with the data using its appropriate data handling method. 
- Generally, the multiple imputation technique is seen as the most used and best approach for dealing with missing data.
- Moving forward, one should always take care to 
   1) **prevent missing data**, i.e. during data collection
   2) **understand its nature**, 
   3) **understand the usage of the data**, 
   4) **understand the mechanisms of data handling methods**, and
   5) **always check the data after handling**.

---

---

---


## CHAIN Data with Confounding Model

## Missing Data Concept and Handling

- What is missing data and why does it occur?
- Missing data mechanisms: MCAR, MAR, and MNAR.
- Ad-hoc approaches to handling missing data and their associated issues.

---

## Multiple Imputation (MI) Method

- Steps in performing MI for cross-sectional data.
- Algorithm with a complex model for MI.
- Pros and cons of using MI compared to ad-hoc methods.

---

## Lab: mice package in R for MAR-based Missing Data Handling

- Introduction to the CHAIN dataset as a real-world example.
- Explanation of why the data is missing and the assumed mechanism (MAR).
- Demonstration of how to use the mice package in R for multiple imputation.
- Performing MI on the CHAIN dataset to handle missing data.

---

## Example: CHAIN Data

- The CHAIN dataset is a subset of the Community Health Advisory and Information Network (CHAIN) study.
- This study is a longitudinal cohort study of people living with HIV in New York City and is conducted by Columbia University School of Public Health (Messeri et al. 2003).
- The CHAIN cohort was recruited in 1994 from a large number of medical care and social service agencies serving HIV in New York City. Cohort members were interviewed up to 8 times through 2002. A total of 532 CHAIN participants completed at least one interview at either the 1st, 2nd, or 3rd rounds of the interview.
- For simplicity, our analysis here discards the time aspect of the dataset and uses only the first round of the survey.

---

## Example: CHAIN Data (cont'd) â€“ Read into R (from SPSS data)

```{r, eval=FALSE}
library("haven")
setwd("~/Desktop/BIOS234/Lab9_10.28")
chain <- read_spss(file = "CHAIN.sav")
dim(chain) # [1] 532 8
head(chain) # A tibble: 6 x 8
```


---

## Outline

â€¢ What is Missing Data
   - Definition of Missing Data
   - Missing Data Patterns
   - Missing Data Mechanisms

â€¢ Simple Ways of Dealing with Missing Data

â€¢ Multiple Imputation of Missing Data
---

## Introduction

- Missing data is very common in statistical analysis. 

- However, having too much missing data can invalidate a study as it reduces power and precision. 

- Dealing with missing data requires considering the missing data patterns, mechanisms, and the chosen analytic approach.

**Common Types of Missing Data**

- Survey nonresponse
- Missing dependent variables
- Missing covariates
- Dropouts
- Censoring - administrative, competing events, LTFU
- Non-reporting or delayed reporting
- Non-compliance
- Measurement error

---

## Implications of Missing Data

- Unbalanced data

- Loss of information and reduced inference

- Extent depends on the amount of missing data, missing pattern, mechanism, parameters of interest, and analysis method

- Care is needed to avoid biased inferences

## Missing Data Mechanisms

- Missing Completely At Random (MCAR)
- Missing At Random (MAR)
- Missing Not At Random (MNAR)

---

### Examples of Missing Data Mechanisms

- Missing Completely At Random (MCAR)
- Missing At Random (MAR)
- Missing Not At Random (MNAR)

---

## Implications of Missing Data Mechanisms

- Missing Completely At Random (MCAR): Complete Case (CC) analysis is acceptable

- Missing At Random (MAR): No CC; Likelihood-based methods and Generalized Estimating Equations (GEE) are okay

- Missing Not At Random (MNAR): No CC; analysis is difficult - use sensitivity analysis

## Missing Data Patterns for Longitudinal Data

- Monotone/dropout missing data pattern: Missing data after the dropout, common in longitudinal studies

- Intermittent/Arbitrary missing data pattern: Missing data before some non-missing data, common in non-longitudinal studies

---

## Common Techniques for Dealing with Missing Data

1. Complete Case Analysis (Listwise deletion)
2. Available Case Analysis (Pairwise deletion)
3. Unconditional Mean Imputation (Mean substitution)
4. Single Imputation (Deterministic Imputation)
5. Stochastic Imputation

---

## Multiple Imputation

- Multiply impute "m" pseudo-complete data sets, typically using a small number of imputations (e.g., 5 < m < 10).

- Combine the inferences from each of the m data sets to acknowledge the uncertainty in the imputation process and the missing data mechanism.

## Multiple Imputation: Combining Inferences

- Combine m sets of parameter estimates to provide a single estimate of the parameter of interest.

- Combine uncertainties to obtain valid standard errors.

- Within-imputation variance and between-imputation variance.

---

## Multiple Imputation versus Likelihood Analysis when Data are MAR

- Both multiple imputation and likelihood analysis are valid when data are MAR.

- The choice between them depends on the efficiency and complexity of the models.

## What if You Doubt the MAR Assumption?

- Methods for Non-MAR (NMAR) data exist, but they require information and assumptions on pr(Missing I observed, unobserved).

- Sensitivity analysis can assess the stability of findings under various scenarios, setting bounds on the form and strength of the dependence.

---

## Summary on Multiple Imputation

- Assumes MAR and performs well in most cases.

- Method depends on the missing data pattern - intermittent or monotone dropout.

- Markov Chain Monte-Carlo (MCMC) method used for intermittent data.

- Propensity scores method (using weights) can be used for monotone missing data pattern.

---

## Thank You

---

Sure, I can provide you with an RMarkdown file that explains the covariance structures for longitudinal data analysis. We will cover the following structures: UN (unstructured), CS (compound symmetry), CSH (compound symmetry with heterogeneous variances), AR(1) (first-order autoregressive), and ARH(1) (first-order autoregressive with heterogeneous variances).


# Covariance Structures for Longitudinal Data Analysis

---

## Unstructured (UN) Covariance Structure

The unstructured covariance structure allows for different variances and covariances between any two time points. The covariance matrix Î£ is given by:


\begin{bmatrix}
\sigma_{1}^2 & \sigma_{1,2} & \dots & \sigma_{1,T} \\
\sigma_{2,1} & \sigma_{2}^2 & \dots & \sigma_{2,T} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{T,1} & \sigma_{T,2} & \dots & \sigma_{T}^2
\end{bmatrix}


Where:
- $\sigma_{i}^2$ is the variance at time point i.
- $\sigma_{i,j}$ is the covariance between time points i and j.
- $T$ is the total number of time points.

---

## Compound Symmetry (CS) Covariance Structure

The compound symmetry covariance structure assumes constant variances at each time point and equal covariances between all pairs of time points. The covariance matrix Î£ is given by:

\begin{bmatrix}
\sigma^2 & \rho & \dots & \rho \\
\rho & \sigma^2 & \dots & \rho \\
\vdots & \vdots & \ddots & \vdots \\
\rho & \rho & \dots & \sigma^2
\end{bmatrix}

Where:
- $\sigma^2$ is the common variance across all time points.
- $\rho$ is the common covariance between any two time points.

---

## Compound Symmetry with Heterogeneous Variances (CSH) Covariance Structure

The compound symmetry with heterogeneous variances covariance structure extends the compound symmetry structure by allowing different variances at each time point, while maintaining equal covariances between all pairs of time points. The covariance matrix Î£ is given by:

\begin{bmatrix}
\sigma_{1}^2 & \rho & \dots & \rho \\
\rho & \sigma_{2}^2 & \dots & \rho \\
\vdots & \vdots & \ddots & \vdots \\
\rho & \rho & \dots & \sigma_{T}^2
\end{bmatrix}


Where:
- $\sigma_{i}^2$ is the variance at time point i.
- $\rho$ is the common covariance between any two time points.

---

## Autoregressive (AR(1)) Covariance Structure

The autoregressive covariance structure assumes that the correlation between two time points decreases exponentially as the time lag between them increases. The covariance matrix Î£ is given by:

\begin{bmatrix}
\sigma_{1}^2 & \sigma_{1,2} & \dots & \sigma_{1,T} \\
\sigma_{1,2} & \sigma_{2}^2 & \dots & \sigma_{2,T} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{1,T} & \sigma_{2,T} & \dots & \sigma_{T}^2
\end{bmatrix}

Where:
- $\sigma_{i}^2$ is the variance at time point i.
- $\sigma_{i,j} = \rho \times \sigma_{i-1} \times \sigma_{j-1}$ is the covariance between time points i and j, with $\rho$ as the autocorrelation parameter.

---

## Autoregressive with Heterogeneous Variances (ARH(1)) Covariance Structure

The autoregressive with heterogeneous variances covariance structure extends the autoregressive structure by allowing different variances at each time point. The covariance matrix Î£ is given by:

\begin{bmatrix}
\sigma_{1}^2 & \sigma_{1,2} & \dots & \sigma_{1,T} \\
\sigma_{1,2} & \sigma_{2}^2 & \dots & \sigma_{2,T} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{1,T} & \sigma_{2,T} & \dots & \sigma_{T}^2
\end{bmatrix}

Where:
- $\sigma_{i}^2$ is the variance at time point $i$.
- $\sigma_{i,j} = \rho \times \sigma_{i-1} \times \sigma_{j-1}$ is the covariance between time points i and j, with $\rho$ as the autocorrelation parameter.

---

These covariance structures play a crucial role in longitudinal data analysis, and the appropriate choice depends on the underlying assumptions and characteristics of the data.
